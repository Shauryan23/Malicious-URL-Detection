{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('phishing_site_urls.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Globals\n",
    "tldSet = set()\n",
    "bad_url_word_frequency = {}\n",
    "good_url_word_frequency = {}\n",
    "sensitive_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_digits(word):\n",
    "  return len(re.findall('[0-9]', word))\n",
    "\n",
    "def count_alphabets(word):\n",
    "  return len(re.findall('[A-z]', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(input_string):\n",
    "  # Pattern to match any non-alphanumeric and non-space character\n",
    "  pattern = r'[^a-zA-Z0-9\\s]'  \n",
    "  return re.sub(pattern, '', input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(word):\n",
    "  if len(word) == 0:\n",
    "    return 0\n",
    "\n",
    "  entropy = 0\n",
    "  word_length = len(word)\n",
    "  char_count = {}\n",
    "\n",
    "  for char in word:\n",
    "    if char in char_count:\n",
    "      char_count[char] += 1\n",
    "    else:\n",
    "      char_count[char] = 1\n",
    "\n",
    "  for char in char_count:\n",
    "    probability = char_count[char] / word_length\n",
    "    entropy += -probability * math.log2(probability)\n",
    "\n",
    "  return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_special_characters(input_string):\n",
    "  special_characters = \"!@#$%^&*()-_=+[]{}|;:',.<>/?\"\n",
    "  count = 0\n",
    "\n",
    "  for char in input_string:\n",
    "    if char not in special_characters and not char.isalnum():\n",
    "      count += 1\n",
    "\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.whoisxmlapi.com/whoisserver/WhoisService?apiKey=at_BGLU4RpM0fcKHpqsqHAYRWFl4bIt2&domainName=facebook.com\n",
    "# Rapid API Key: d2592fe48dmsh190428470dadf9cp1bf762jsnf40816e6115e (Shauryansingh23102001@gmail.com)\n",
    "\n",
    "# FUNCTION TO MAKE REQUEST TO WHOISAPI THROUGH RAPIDAPI (Not in use currently)\n",
    "def whois_api_request(api_key, domain_url):\n",
    "    url = \"https://whoisapi-whois-v2-v1.p.rapidapi.com/whoisserver/WhoisService\"\n",
    "\n",
    "    headers = {\n",
    "\t    \"X-RapidAPI-Key\": \"d2592fe48dmsh190428470dadf9cp1bf762jsnf40816e6115e\",\n",
    "\t    \"X-RapidAPI-Host\": \"whoisapi-whois-v2-v1.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    querystring = {\n",
    "        \"domainName\":domain_url,\n",
    "        \"apiKey\":api_key,\n",
    "        \"outputFormat\":\"JSON\",\n",
    "        \"da\":\"0\",\n",
    "        \"ipwhois\":\"1\",\n",
    "        \"thinWhois\":\"0\",\n",
    "        \"_parse\":\"0\",\n",
    "        \"preferfresh\":\"1\",\n",
    "        \"checkproxydata\":\"0\",\n",
    "        \"ip\":\"1\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for (index, colname) in enumerate(df):\n",
    "  if(index == 2500):\n",
    "    break\n",
    "\n",
    "  URL = df[\"URL\"].values\n",
    "  domain = URL.split(\"/\")[0]\n",
    "  url_length = len(URL)\n",
    "  domain_entropy = calculate_entropy(domain)\n",
    "\n",
    "  # Top Level Domain Extraction Logic\n",
    "  tld = domain.split(\".\")[-1]\n",
    "  tld_length = len(tld)\n",
    "  tld = tld.split(\":\")[0]\n",
    "  tld = tld.split(\"-\")[0]\n",
    "  if(tld_length > 1 and tld_length < 10 and count_alphabets(tld) > count_digits(tld)):\n",
    "    tldSet.add(tld)\n",
    "\n",
    "  if df[\"Label\"].values == \"bad\":\n",
    "    temp = URL.split(\"/\")\n",
    "    subdomain = \"/\".join(temp[1:])\n",
    "    words_list = re.split(\"[/,.,?,-,+,=,&,~,!,@,#,$,%,_,;]\", subdomain)\n",
    "    for word in words_list:\n",
    "      word = word.strip().lower()\n",
    "      word = remove_special_characters(word)\n",
    "      digs = len(re.findall('[0-9]', word))\n",
    "      alps = len(re.findall('[A-z]', word))\n",
    "      if(len(word) < 2 or len(word) > 15 or digs > alps):\n",
    "        continue\n",
    "      if word in bad_url_word_frequency:\n",
    "        bad_url_word_frequency[word] += 1\n",
    "      else:\n",
    "        bad_url_word_frequency[word] = 1\n",
    "  else:\n",
    "    temp = URL.split(\"/\")\n",
    "    subdomain = \"/\".join(temp[1:])\n",
    "    words_list = re.split(\"[/,.,?,-,+,=,&,~,!,@,#,$,%,_,;]\", subdomain)\n",
    "    for word in words_list:\n",
    "      word = word.strip().lower()\n",
    "      word = remove_special_characters(word)\n",
    "      digs = len(re.findall('[0-9]', word))\n",
    "      alps = len(re.findall('[A-z]', word))\n",
    "      if(len(word) < 2 or len(word) > 15 or digs > alps):\n",
    "        continue\n",
    "      if word in good_url_word_frequency:\n",
    "        good_url_word_frequency[word] += 1\n",
    "      else:\n",
    "        good_url_word_frequency[word] = 1\n",
    "\n",
    "print(len(good_url_word_frequency))\n",
    "print(len(bad_url_word_frequency))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAST\n",
    "start_row_idx = 3522\n",
    "end_row_idx = 4002\n",
    "\n",
    "# TO DO\n",
    "start_row_idx = 4003\n",
    "end_row_idx = 4483\n",
    "\n",
    "df = df.iloc[start_row_idx : end_row_idx]\n",
    "print(df.shape[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED WHOIS API FUNCTION\n",
    "\n",
    "def get_whois_data(api_key, domain_name):\n",
    "  base_url = \"https://www.whoisxmlapi.com/whoisserver/WhoisService\"\n",
    "  params = {\n",
    "    \"apiKey\": api_key,\n",
    "    \"domainName\": domain_name,\n",
    "    \"outputFormat\":\"JSON\",\n",
    "    \"preferfresh\":\"1\",\n",
    "  }\n",
    "\n",
    "  response = requests.get(base_url, params=params)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW ONE\n",
    "# Index Starts from 1\n",
    "DATA = []\n",
    "\n",
    "# START = start_row_idx - 1\n",
    "cnt = 0\n",
    "for index, row in df.iterrows():\n",
    "  if(cnt > 500):\n",
    "    break\n",
    "    \n",
    "  URL = row[\"URL\"]\n",
    "  label = row[\"Label\"]\n",
    "\n",
    "  domain = URL.split(\"/\")[0]\n",
    "  url_length = len(URL)\n",
    "  domain_entropy = calculate_entropy(domain)\n",
    "\n",
    "  temp = URL.split(\"/\")\n",
    "  subdomain = temp[0].split(\".\")[-1]\n",
    "  path_rest = \"/\".join(temp[1:])\n",
    "  path_rest_length = len(path_rest)\n",
    "\n",
    "  words_list = re.split(\"[/,.,?,-,+,=,&,~,!,@,#,$,%,_,;]\", path_rest)\n",
    "\n",
    "  tld = domain.split(\".\")[-1]\n",
    "  tld_length = len(tld)\n",
    "  tld = tld.split(\":\")[0]\n",
    "  tld = tld.split(\"-\")[0]\n",
    "\n",
    "  num_spcs_chars = count_special_characters(path_rest)\n",
    "\n",
    "  url = \"https://whoisapi-whois-v2-v1.p.rapidapi.com/whoisserver/WhoisService\"\n",
    "\n",
    "  querystring = {\n",
    "    \"domainName\":domain,\n",
    "    \"apiKey\":\"at_u6XSMmxG3lpSkcRJFsHzt60WqQQUc\",\n",
    "    \"outputFormat\":\"JSON\",\n",
    "    \"da\":\"0\",\n",
    "    \"ipwhois\":\"1\",\n",
    "    \"thinWhois\":\"0\",\n",
    "    \"_parse\":\"0\",\n",
    "    \"preferfresh\":\"1\",\n",
    "    \"checkproxydata\":\"0\",\n",
    "    \"ip\":\"1\"\n",
    "  }\n",
    "\n",
    "  headers = {\n",
    "    \"X-RapidAPI-Key\": \"d2592fe48dmsh190428470dadf9cp1bf762jsnf40816e6115e\",\n",
    "    \"X-RapidAPI-Host\": \"whoisapi-whois-v2-v1.p.rapidapi.com\"\n",
    "  }\n",
    "  \n",
    "  response = requests.get(url, headers=headers, params=querystring)\n",
    "  \n",
    "  response_data = response.json()\n",
    "  \n",
    "  if \"ErrorMessage\" in response_data:\n",
    "    continue\n",
    "\n",
    "  if \"WhoisRecord\" in response_data:\n",
    "    if \"createdDate\" in response_data[\"WhoisRecord\"]:\n",
    "      created_year = response_data[\"WhoisRecord\"][\"createdDate\"][0:4]\n",
    "    elif \"registryData\" in response_data[\"WhoisRecord\"] and \"createdDate\" in response_data[\"WhoisRecord\"][\"registryData\"]:\n",
    "      created_year = response_data[\"WhoisRecord\"][\"registryData\"][\"createdDate\"][0:4]\n",
    "    else:\n",
    "      created_year = None\n",
    "\n",
    "  if \"WhoisRecord\" in response_data:\n",
    "    if \"updatedDate\" in response_data[\"WhoisRecord\"]:\n",
    "      updated_year = response_data[\"WhoisRecord\"][\"updatedDate\"][0:4]\n",
    "    elif \"registryData\" in response_data[\"WhoisRecord\"] and \"updatedDate\" in response_data[\"WhoisRecord\"][\"registryData\"]:\n",
    "      updated_year = response_data[\"WhoisRecord\"][\"registryData\"][\"updatedDate\"][0:4]\n",
    "    else:\n",
    "      updated_year = None\n",
    "\n",
    "  if \"WhoisRecord\" in response_data:\n",
    "    if \"expiresDate\" in response_data[\"WhoisRecord\"]:\n",
    "      expires_year = response_data[\"WhoisRecord\"][\"expiresDate\"][0:4]\n",
    "    elif \"registryData\" in response_data[\"WhoisRecord\"] and \"expiresDate\" in response_data[\"WhoisRecord\"][\"registryData\"]:\n",
    "      expires_year = response_data[\"WhoisRecord\"][\"registryData\"][\"expiresDate\"][0:4]\n",
    "    else:\n",
    "      expires_year = None\n",
    "\n",
    "  if \"WhoisRecord\" in response_data:\n",
    "    if (\"registrant\" in response_data[\"WhoisRecord\"] and \n",
    "      \"countryCode\" in response_data[\"WhoisRecord\"][\"registrant\"]):\n",
    "      country_code = response_data[\"WhoisRecord\"][\"registrant\"][\"countryCode\"]\n",
    "    elif (\n",
    "      \"registryData\" in response_data[\"WhoisRecord\"] and\n",
    "      \"registrant\" in response_data[\"WhoisRecord\"][\"registryData\"] and\n",
    "      \"countryCode\" in response_data[\"WhoisRecord\"][\"registryData\"][\"registrant\"]):\n",
    "      country_code = response_data[\"WhoisRecord\"][\"registryData\"][\"registrant\"][\"countryCode\"]\n",
    "    else:\n",
    "      country_code = None\n",
    "      \n",
    "  if \"WhoisRecord\" in response_data and \"estimatedDomainAge\" in response_data[\"WhoisRecord\"]:\n",
    "    domain_age = response_data[\"WhoisRecord\"][\"estimatedDomainAge\"]\n",
    "  else:\n",
    "    domain_age = None\n",
    "\n",
    "  df2_data = [subdomain, domain, tld, path_rest, url_length, path_rest_length, num_spcs_chars, domain_entropy, domain_age, created_year, updated_year, expires_year, country_code]\n",
    "\n",
    "  cnt += 1\n",
    "  DATA.append(df2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(DATA, columns=[\"subdomain\", \"domain\", \"tld\", \"path_rest\", \"url_length\", \"path_rest_length\", \"num_spcs_chars\", \"domain_entropy\", \"domain_age\", \"created_year\", \"updated_year\", \"expires_year\", \"country_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('df2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.reindex(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT COLUMNS IN BETWEEN THE DATAFRAME\n",
    "insert_position = df.columns.get_loc('URL') + 1\n",
    "\n",
    "for col_name in df2.columns[::-1]:\n",
    "  df.insert(insert_position, col_name, df2[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('newData/url_4003_to_4483.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Iterate all rows using DataFrame.iterrows() \n",
    "for index, row in df.iterrows():\n",
    "  print(index)\n",
    "  if(index == 25):\n",
    "    break\n",
    "\n",
    "  URL = row[\"URL\"]\n",
    "  label = row[\"Label\"]\n",
    "  domain = URL.split(\"/\")[0]\n",
    "  url_length = len(URL)\n",
    "  domain_entropy = calculate_entropy(domain)\n",
    "\n",
    "  # Top Level Domain Extraction Logic\n",
    "  tld = domain.split(\".\")[-1]\n",
    "  tld_length = len(tld)\n",
    "  tld = tld.split(\":\")[0]\n",
    "  tld = tld.split(\"-\")[0]\n",
    "  if(tld_length > 1 and tld_length < 10 and count_alphabets(tld) > count_digits(tld)):\n",
    "    tldSet.add(tld)\n",
    "\n",
    "  temp = URL.split(\"/\")\n",
    "  subdomain = \"/\".join(temp[1:])\n",
    "  words_list = re.split(\"[/,.,?,-,+,=,&,~,!,@,#,$,%,_,;]\", subdomain)\n",
    "  \n",
    "  # Create Word Frequency for Good and Bad URLs\n",
    "  if label == \"bad\":\n",
    "    for word in words_list:\n",
    "      word = word.strip().lower()\n",
    "      word = remove_special_characters(word)\n",
    "      digs = len(re.findall('[0-9]', word))\n",
    "      alps = len(re.findall('[A-z]', word))\n",
    "      \n",
    "      if(len(word) > 2 or len(word) < 15 or digs < alps):  \n",
    "        if word in bad_url_word_frequency:\n",
    "          bad_url_word_frequency[word] += 1\n",
    "        else:\n",
    "          bad_url_word_frequency[word] = 1\n",
    "  else:\n",
    "    for word in words_list:\n",
    "      word = word.strip().lower()\n",
    "      word = remove_special_characters(word)\n",
    "      digs = len(re.findall('[0-9]', word))\n",
    "      alps = len(re.findall('[A-z]', word))\n",
    "      \n",
    "      if(len(word) > 2 or len(word) < 15 or digs < alps):\n",
    "        if word in good_url_word_frequency:\n",
    "          good_url_word_frequency[word] += 1\n",
    "        else:\n",
    "          good_url_word_frequency[word] = 1\n",
    "\n",
    "sorted_good_url_word_frequency = sorted(good_url_word_frequency.items(), key=lambda x:x[1], reverse=True)\n",
    "sorted_bad_url_word_frequency = sorted(bad_url_word_frequency.items(), key=lambda x:x[1], reverse=True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in good_url_word_frequency:\n",
    "  print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in bad_url_word_frequency:\n",
    "    print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in good_url_word_frequency:\n",
    "  if(it[1] > 5):\n",
    "    sensitive_words.append(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in bad_url_word_frequency:\n",
    "  if(it[1] > 5):\n",
    "    sensitive_words.append(it)\n",
    "\n",
    "#### *** Export Sensitive Words to a file. Only the words sensitive_words[0] *** ###\n",
    "#### *** Loop through good label rows to export their TLD to a file *** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top_level_domains.txt', 'w', encoding='utf-8') as file:\n",
    "    for item in tldSet:\n",
    "        file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as file:\n",
    "  for item in sensitive_words:\n",
    "    file.write(str(item) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
